Reclaim Policies in Kubernetes
Kubernetes supports three types of PersistentVolume (PV) reclaim policies that define what happens to the volume and its underlying storage when the bound PersistentVolumeClaim (PVC) is deleted.

1. Retain
Behavior: When a PVC is deleted, the associated PV is not deleted and enters a Released state.

Purpose: Useful for manual recovery or investigation. The actual data remains intact, allowing administrators to manually reclaim or reuse it.

Use case: Critical data that shouldn't be lost automatically.

2. Delete
Behavior: When a PVC is deleted, the associated PV and the underlying storage (e.g., EBS volume, NFS share) are automatically deleted.

Purpose: Ensures storage is cleaned up when no longer needed.

Use case: Temporary or test environments where automatic cleanup is preferred.

3. Recycle (Deprecated)
Behavior: Used to scrub (wipe) the volume and make it available again.

Status: Deprecated in favor of explicit, custom cleanup logic or dynamic provisioning.

Example: Static NFS PersistentVolume
yaml
Copy
Edit
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 172.31.11.218
    path: /mnt/nfs_share
  persistentVolumeReclaimPolicy: Delete  # Options: Retain | Delete
Dynamic NFS Volume Provisioning Using External Provisioner
This example shows how to set up dynamic provisioning using the NFS Subdir External Provisioner.

Step 1: RBAC for Provisioner
yaml
Copy
Edit
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-pod-provisioner-sa
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nfs-provisioner-clusterRole
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-rolebinding
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-clusterRole
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-pod-provisioner-otherRoles
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: nfs-pod-provisioner-sa
    namespace: kube-system
roleRef:
  kind: Role
  name: nfs-pod-provisioner-otherRoles
  apiGroup: rbac.authorization.k8s.io
Step 2: Deploy NFS Provisioner
yaml
Copy
Edit
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-pod-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-pod-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-pod-provisioner
    spec:
      serviceAccountName: nfs-pod-provisioner-sa
      containers:
        - name: nfs-pod-provisioner
          image: rkevin/nfs-subdir-external-provisioner:fix-k8s-1.20
          volumeMounts:
            - name: nfs-provisioner-v
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: nfs-provisioner
            - name: NFS_SERVER
              value: 172.31.11.218
            - name: NFS_PATH
              value: /mnt/nfs_share
      volumes:
        - name: nfs-provisioner-v
          nfs:
            server: 172.31.11.218
            path: /mnt/nfs_share
Step 3: Create StorageClass
yaml
Copy
Edit
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storageclass
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: nfs-provisioner
parameters:
  archiveOnDelete: "false"
✅ Apply with:
kubectl apply -f nfsProvisioner.yaml

MongoDB ReplicaSet with NFS PVC
yaml
Copy
Edit
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
  namespace: prod
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
        - name: mongocon
          image: mongo
          ports:
            - containerPort: 27017
          env:
            - name: MONGO_INITDB_ROOT_USERNAME
              value: devdb
            - name: MONGO_INITDB_ROOT_PASSWORD
              value: devdb@123
          volumeMounts:
            - name: mongonfsvol
              mountPath: /data/db
      volumes:
        - name: mongonfsvol
          persistentVolumeClaim:
            claimName: mongodb-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: prod
spec:
  type: ClusterIP
  selector:
    app: mongodb
  ports:
    - port: 27017
      targetPort: 27017
Spring Boot Deployment with MongoDB Connection (Different Namespace)
yaml
Copy
Edit
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: test-ns
spec:
  replicas: 2
  selector:
    matchLabels:
      app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
        - name: springapp
          image: kkeducation12345/spring-app:1.0.0
          ports:
            - containerPort: 8080
          env:
            - name: MONGO_DB_HOSTNAME
              value: mongosvc.prod.svc.cluster.local  # Use full DNS if accessing cross-namespace
            - name: MONGO_DB_USERNAME
              value: devdb
            - name: MONGO_DB_PASSWORD
              value: devdb@123
          resources:
            requests:
              cpu: 300m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
  namespace: test-ns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
    - port: 80
      targetPort: 8080



✅ Basic Interview Questions
Q1. What is a reclaim policy in Kubernetes?
Answer:
A reclaim policy in Kubernetes determines what happens to a PersistentVolume (PV) when its associated PersistentVolumeClaim (PVC) is deleted. The reclaim policy is defined in the PV specification using the persistentVolumeReclaimPolicy field.

Q2. What are the different types of reclaim policies in Kubernetes?
Answer:
There are three reclaim policies:

Retain – Keeps the volume and data after PVC is deleted.

Delete – Deletes the volume and underlying storage.

Recycle (Deprecated) – Used to scrub the volume and make it available again. This has been deprecated.

Q3. When should you use the ‘Retain’ reclaim policy?
Answer:
Use Retain when you want to preserve data after a PVC is deleted — for example:

In production environments where data loss must be avoided.

When manual cleanup or migration is required.

For compliance or forensic use cases.

Q4. What happens when a PVC bound to a PV with Retain is deleted?
Answer:
The PVC is deleted, but the PV remains in the "Released" state, and the actual data is not deleted. An admin must manually delete or recycle the PV before it can be reused.

Q5. What is the difference between ‘Delete’ and ‘Retain’ reclaim policies?
Answer:

Feature	Delete	Retain
Action on PVC delete	Deletes PV and data	Keeps PV and data
Reusability	Storage is freed and reused	Admin intervention required
Risk	Data is permanently lost	Data is preserved
Use Case	Test/temporary environments	Production, backup, audits

✅ Scenario-Based Questions
Q6. Scenario: You have a critical database volume. What reclaim policy would you use and why?
Answer:
I would use the Retain reclaim policy to ensure that data is not automatically deleted if the PVC is deleted accidentally. This gives us the chance to back up or inspect the data before manual deletion.

Q7. You noticed a PV in ‘Released’ state. What could be the reason, and what actions will you take?
Answer:
Reason: The PVC using the PV was deleted and the PV has a Retain policy.

Action:

Manually clean or inspect the data.

Delete or update the PV manually.

If reusing, remove the claimRef and bind it to a new PVC.

Q8. Can you change the reclaim policy of an existing PV?
Answer:
Yes, you can update the reclaim policy using:

bash
Copy
Edit
kubectl patch pv <pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
But this only affects future behavior, not existing bindings.

Q9. What happens to dynamically provisioned volumes when the PVC is deleted?
Answer:
Dynamically provisioned volumes typically use the Delete reclaim policy (defined in the StorageClass), so both the PV and the underlying storage (e.g., AWS EBS, GCE disk, NFS share) are automatically deleted.

Q10. How does reclaim policy impact storage costs in cloud environments?
Answer:
If Retain is used, storage may continue to be billed even if it's not in use. It requires manual cleanup. With Delete, storage is released automatically, saving cost in test environments.


Ask ChatGPT


