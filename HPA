üìå What is HPA in Kubernetes?
HPA (Horizontal Pod Autoscaler) is a Kubernetes controller that automatically adjusts the number of pod replicas in a Deployment, ReplicaSet, or StatefulSet based on observed CPU/memory utilization or other custom metrics.

‚úÖ When Do We Use HPA?
Use HPA when your application's workload is variable or unpredictable, and you want to scale automatically based on resource consumption.
‚öôÔ∏è How It Works
HPA continuously monitors metrics (default is CPU usage via Metrics Server) and:

Increases pod replicas when usage is high

Decreases pod replicas when usage is low

üß™ Example Use Case
Let‚Äôs say you have a web application that should maintain CPU usage at 50%. You want to scale between 2 and 10 pods based on that metric.

1. vertical
     2. Horizontal

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50


kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml



metric server set-up
====================

--> search metric server yaml for hpa

--> open the GitHub link

--> kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

--> kubectl get all -n kube-system

--> But pod is not coming up due to certificate issue.

--> kubectl edit deploy metrics-server -n kube-system

--> 

spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls  ---> add this line


=========================================================================

#deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
  labels:
    name: hpadeployment
spec:
  replicas: 1
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
      - name: hpacontainer
        image: k8s.gcr.io/hpa-example
        ports:
        - name: http
          containerPort: 80
        resources:
          requests:
            cpu: "100m"
            memory: "64Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
---
#HPA

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpadeploymentautoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpadeployment
  minReplicas: 2
  maxReplicas: 4
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 30
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

---

#service

apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
  - port: 80
    targetPort: 80
  selector:
    name: hpapod
  type: ClusterIP


========================================================


# ==== Execute below commands to increase load====

Now, we will see how the auto scaler reacts to increased load. We will start a container, and send an infinite loop of queries to the php-apache service .
# Create A Temp Pod in interactive mod to access app using service name

  $ kubectl run -i --tty load-generator --rm --image=busybox /bin/sh	
# Execute below command in Temp Pod
 
  $ while true; do wget -q -O- http://hpaclusterservice; done	

Open kubectl terminal in another tab and watch kubectl get pods or kubect get hpa to see how the auto scaler reacts to increased load.

  $ watch kubectl get hpa

3. Apply HPA
bash
Copy
Edit
kubectl autoscale deployment web-app --cpu-percent=50 --min=2 --max=10
This command creates an HPA that:

Keeps CPU usage around 50%

| Category                                 | Issue / Challenge                             | Explanation                                                                                                                             |
| ---------------------------------------- | --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| üß© **Metrics**                           | Metrics server not installed or misconfigured | HPA needs resource metrics (like CPU, memory). If `metrics-server` isn't running or isn't exposing metrics properly, HPA won't work.    |
| ‚è±Ô∏è **Slow Reaction**                     | HPA responds slowly to spikes                 | HPA reacts every 15 seconds by default, and scaling may take a minute or more ‚Äî not ideal for sudden traffic bursts.                    |
| üìâ **Scaling Down Aggressively**         | Too many scale-downs                          | When traffic drops suddenly, HPA might aggressively scale down, causing 502/503 errors if traffic returns quickly.                      |
| üß™ **Testing HPA**                       | Hard to simulate load                         | Generating controlled, measurable CPU/memory load in non-prod environments for HPA testing is difficult.                                |
| üîÑ **Pod startup time**                  | Pods take time to initialize                  | Even if HPA scales out fast, the application in new pods might take time to boot (e.g., Java apps), delaying readiness.                 |
| ‚ùå **Wrong resource requests/limits**     | Misconfigured CPU/memory                      | HPA depends on resource requests/limits. If not set properly, autoscaling won't trigger correctly.                                      |
| üìä **Limited to CPU/Memory by default**  | Custom metrics need extra setup               | Out-of-the-box, HPA only supports CPU/memory. Using custom metrics (like queue length, latency) requires Prometheus Adapter or similar. |
| üì¶ **Stateless Only**                    | Not suitable for stateful apps                | HPA is not suitable for apps with persistent connections or local state (e.g., Redis, databases).                                       |
| üîÅ **Conflicts with Cluster Autoscaler** | HPA scales pods, CA scales nodes              | If HPA adds pods but no nodes are available, it may fail unless Cluster Autoscaler is configured correctly.                             |
| üßÆ **Wrong target values**               | Poorly tuned target thresholds                | If `--cpu-percent=50` is too low or high for your workload, it may scale too often or not at all.                                       |


üìò Real-Time Scenarios
‚úÖ Scenario 1: Web App Scaling with Traffic Surge
Context:
E-commerce app receives high traffic during a sale.

Challenge:

CPU usage increases rapidly.

HPA begins scaling but new pods take 30 seconds to be ready.

Some requests fail before the new pods are live.

Solution:

Pre-scale before the event using a scheduled job.

Use readiness probes properly to avoid routing traffic to initializing pods.

Tune resource limits and HPA thresholds.

‚úÖ Scenario 2: Low Load During Off Hours
Context:
Company dashboard runs 24x7 but used mostly during working hours.

Challenge:

Running at 10 pods overnight is wasteful.

Need to scale down to minimum automatically.

Solution:

Use HPA with minPods set to 2.

Observed memory usage low ‚Üí HPA scales down to save cost.

‚úÖ Scenario 3: API Scaling with Queue Length
Context:
An API processes messages from a queue (like RabbitMQ or Kafka).

Challenge:

CPU is low but the queue keeps growing.

HPA (CPU-based) doesn‚Äôt scale even though the system is overloaded.

Solution:

Set up custom metrics using Prometheus Adapter.

Configure HPA to scale based on queue depth.

‚úÖ Scenario 4: Misconfigured Resource Requests
Context:
Microservice has no CPU request/limit defined.

Challenge:

HPA doesn‚Äôt scale because it sees 0% CPU usage (since no baseline is defined).

Solution:

Define appropriate resources.requests.cpu in Deployment YAML.

Example:

yaml
Copy
Edit
resources:
  requests:
    cpu: 200m
  limits:
    cpu: 500m
‚úÖ Scenario 5: Cluster Out of Resources
Context:
HPA scales up pods, but they stay in Pending.

Challenge:

No available nodes in the cluster to run the extra pods.

HPA shows more replicas, but pods are stuck.

Solution:

Integrate with Cluster Autoscaler to add more nodes when required.

Monitor node resource usage actively.

üß† Pro Tips
Always test HPA in staging before using it in production.

Combine HPA with Cluster Autoscaler for best results.

Use Prometheus Adapter for custom scaling metrics.

Avoid HPA for stateful or long-lived connections.

Monitor HPA behavior using:

b

